{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VARIABLE NAME PREDICTION WITH CODE-TRANSFORMER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pojer/adv-ml/code-transformer\n"
     ]
    }
   ],
   "source": [
    "%cd /home/pojer/adv-ml/code-transformer/\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pojer/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from code_transformer.preprocessing.datamanager.preprocessed import CTPreprocessedDataManager\n",
    "from code_transformer.preprocessing.graph.binning import ExponentialBinning\n",
    "from code_transformer.preprocessing.graph.distances import PersonalizedPageRank, ShortestPaths, \\\n",
    "    AncestorShortestPaths, SiblingShortestPaths, DistanceBinning\n",
    "from code_transformer.preprocessing.graph.transform_var import DistancesTransformerVar\n",
    "from code_transformer.preprocessing.nlp.vocab import VocabularyTransformer, CodeSummarizationVocabularyTransformer\n",
    "\n",
    "from code_transformer.preprocessing.pipeline.stage1var import CTStage1VarPreprocessor \n",
    "\n",
    "from code_transformer.preprocessing.pipeline.stage2var import CTStage2VarMultiLanguageSample\n",
    "from code_transformer.utils.inference_var import get_model_manager, make_batch_from_sample, decode_predicted_tokens\n",
    "\n",
    "\n",
    "from code_transformer.env import DATA_PATH_STAGE_2\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Load the model with the name of the folder and the its type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained on: python\n"
     ]
    }
   ],
   "source": [
    "model_type = 'code_transformer'  # code_transformer, great or xl_net\n",
    "# model_type = 'great' \n",
    "run_id = 'CT-188-var'  # Name of folder in which snapshots are stored\n",
    "# run_id = 'GT-2'\n",
    "snapshot = 'latest'  # Use 'latest' for the last stored snapshot\n",
    "model_manager = get_model_manager(model_type)\n",
    "model_config = model_manager.load_config(run_id)\n",
    "\n",
    "language = model_config['data_setup']['language']\n",
    "print(f\"Model was trained on: {language}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model state and set in evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_manager.load_model(run_id, snapshot, gpu=False)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the funtion and set the variable to predit\n",
    "The code snippet to send to the transformer, to predict a specific variable inside the function just write an 'a' to each instance of this variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_snippet = \"\"\"\n",
    "def send_command(self, a, as_list=False):\n",
    "  action = actions.Action({\n",
    "      'Command': a,\n",
    "      'Action': 'Command'},\n",
    "    as_list=as_list)\n",
    "  return self.send_action(action)\n",
    "\"\"\"\n",
    "code_snippet_language = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_snippet = \"\"\"\n",
    "def to_bytes(a, encoding='utf-8'):\n",
    "  if not a:\n",
    "    return a\n",
    "  if not isinstance(a, bytes_type):\n",
    "    a = a.encode(encoding)\n",
    "  return a\n",
    "\"\"\"\n",
    "code_snippet_language = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "code_snippet = \"\"\"\n",
    "def ensure_directory(path):\n",
    "  a = os.path.dirname(path)\n",
    "  if not os.path.isdir(a):\n",
    "    os.makedirs(a)\n",
    "\"\"\"\n",
    "code_snippet_language = 'python\n",
    "\n",
    "code_snippet = \"\"\"\n",
    "def ensure_directory(path):\n",
    "  a = os.path.dirname(path)\n",
    "  if not os.path.isdir(a):\n",
    "    os.makedirs(a)\n",
    "\"\"\"\n",
    "code_snippet_language = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "code_snippet = \"\"\"\n",
    "def read_data(self, f):\n",
    "  '''\n",
    "  Read the data and populate the vocabulary\n",
    "  '''\n",
    "  file = open(f, 'r', encoding='utf-8').readlines()\n",
    "  r_file = []\n",
    "  for line in file:\n",
    "    tmp = line.strip().split()\n",
    "    r_file.append(tmp)\n",
    "    # if there are more words in test and valid we add to voc\n",
    "    self.voc.add_word_sent(tmp)\n",
    "\n",
    "  return r_file\n",
    "\"\"\"\n",
    "\n",
    "code_snippet = \"\"\"\n",
    "def read_data(self, f):\n",
    "  '''\n",
    "  Read the data and populate the vocabulary\n",
    "  '''\n",
    "  file = open(f, 'r', encoding='utf-8').readlines()\n",
    "  r_file = []\n",
    "  for a in file:\n",
    "    tmp = a.strip().split()\n",
    "    r_file.append(tmp)\n",
    "    # if there are more words in test and valid we add to voc\n",
    "    self.voc.add_word_sent(tmp)\n",
    "\n",
    "  return r_file\n",
    "\"\"\"\n",
    "code_snippet_language = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "code_snippet = \"\"\"\n",
    "def init_weights(self):\n",
    "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "    for weight in self.parameters():\n",
    "        weight.data.uniform_(-stdv, stdv)\n",
    "\"\"\"\n",
    "\n",
    "code_snippet = \"\"\"\n",
    "def init_weights(self):\n",
    "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "    for a in self.parameters():\n",
    "        a.data.uniform_(-stdv, stdv)\n",
    "\"\"\"\n",
    "\n",
    "code_snippet_language = 'python'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAGE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = CTStage1VarPreprocessor(code_snippet_language, allow_empty_methods=True)\n",
    "stage1_sample = preprocessor.process([(\"f\", \"\", code_snippet)], 0, \"a\", interactive=True)\n",
    "# print(stage1_sample[0].variable_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAGE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config of the respective dataset that this model was trained on\n",
    "model_language = model_config['data_setup']['language']\n",
    "data_manager = CTPreprocessedDataManager(DATA_PATH_STAGE_2, model_language, partition='train', shuffle=True)\n",
    "data_config = data_manager.load_config()\n",
    "\n",
    "# Extract how distances should be computed from the dataset config\n",
    "distances_config = data_config['distances']\n",
    "PPR_ALPHA = distances_config['ppr_alpha']\n",
    "PPR_USE_LOG = distances_config['ppr_use_log']\n",
    "PPR_THRESHOLD = distances_config['ppr_threshold']\n",
    "\n",
    "SP_THRESHOLD = distances_config['sp_threshold']\n",
    "\n",
    "ANCESTOR_SP_FORWARD = distances_config['ancestor_sp_forward']\n",
    "ANCESTOR_SP_BACKWARD = distances_config['ancestor_sp_backward']\n",
    "ANCESTOR_SP_NEGATIVE_REVERSE_DISTS = distances_config['ancestor_sp_negative_reverse_dists']\n",
    "ANCESTOR_SP_THRESHOLD = distances_config['ancestor_sp_threshold']\n",
    "\n",
    "SIBLING_SP_FORWARD = distances_config['sibling_sp_forward']\n",
    "SIBLING_SP_BACKWARD = distances_config['sibling_sp_backward']\n",
    "SIBLING_SP_NEGATIVE_REVERSE_DISTS = distances_config['sibling_sp_negative_reverse_dists']\n",
    "SIBLING_SP_THRESHOLD = distances_config['sibling_sp_threshold']\n",
    "\n",
    "# Extract how distances should be binned from the dataset config\n",
    "binning_config = data_config['binning']\n",
    "EXPONENTIAL_BINNING_GROWTH_FACTOR = binning_config['exponential_binning_growth_factor']\n",
    "N_FIXED_BINS = binning_config['n_fixed_bins']\n",
    "NUM_BINS = binning_config['num_bins']\n",
    "\n",
    "preprocessing_config = data_config['preprocessing']\n",
    "REMOVE_PUNCTUATION = preprocessing_config['remove_punctuation']\n",
    "\n",
    "# Put together all the implementations of the different distance metrics\n",
    "distance_metrics = [\n",
    "    PersonalizedPageRank(threshold=PPR_THRESHOLD, log=PPR_USE_LOG, alpha=PPR_ALPHA),\n",
    "    ShortestPaths(threshold=SP_THRESHOLD),\n",
    "    AncestorShortestPaths(forward=ANCESTOR_SP_FORWARD, backward=ANCESTOR_SP_BACKWARD,\n",
    "                          negative_reverse_dists=ANCESTOR_SP_NEGATIVE_REVERSE_DISTS,\n",
    "                          threshold=ANCESTOR_SP_THRESHOLD),\n",
    "    SiblingShortestPaths(forward=SIBLING_SP_FORWARD, backward=SIBLING_SP_BACKWARD,\n",
    "                         negative_reverse_dists=SIBLING_SP_NEGATIVE_REVERSE_DISTS,\n",
    "                         threshold=SIBLING_SP_THRESHOLD)]\n",
    "\n",
    "db = DistanceBinning(NUM_BINS, N_FIXED_BINS, ExponentialBinning(EXPONENTIAL_BINNING_GROWTH_FACTOR))\n",
    "\n",
    "distances_transformer = DistancesTransformerVar(distance_metrics, db)\n",
    "vocabs = data_manager.load_vocabularies()\n",
    "if len(vocabs) == 4:\n",
    "    vocabulary_transformer = CodeSummarizationVocabularyTransformer(*vocabs)\n",
    "else:\n",
    "    vocabulary_transformer = VocabularyTransformer(*vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, take the result of stage1 preprocessing and feed it through the vocabulary and distances transformer to obtain a stage2 sample\n",
    "stage2_sample = stage1_sample[0]\n",
    "if REMOVE_PUNCTUATION:\n",
    "    stage2_sample.remove_punctuation()\n",
    "stage2_sample = vocabulary_transformer(stage2_sample)\n",
    "stage2_sample = distances_transformer(stage2_sample)\n",
    "\n",
    "if ',' in model_language:\n",
    "    # In the multi-lingual setting, we have to furthermore bake the code snippet language into the sample\n",
    "    stage2_sample = CTStage2VarMultiLanguageSample(stage2_sample.tokens, stage2_sample.graph_sample, stage2_sample.token_mapping,\n",
    "                                                stage2_sample.stripped_code_snippet, stage2_sample.func_name,\n",
    "                                                stage2_sample.docstring,\n",
    "                                                code_snippet_language,\n",
    "                                                encoded_func_name=stage2_sample.encoded_func_name if hasattr(stage2_sample, 'encoded_func_name') else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = make_batch_from_sample(stage2_sample, model_config, model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.forward_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "predictions = output.logits \\\n",
    "    .topk(k, axis=-1)\\\n",
    "    .indices\\\n",
    "    .squeeze()\\\n",
    "    .T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted variable names:\n",
      "  (1)  line\n",
      "  (2)  f sent body\n",
      "  (3)  chunk file splitted\n",
      "  (4)  r name\n",
      "  (5)  file line 256\n"
     ]
    }
   ],
   "source": [
    "print('Predicted variable names:')\n",
    "for i, prediction in enumerate(predictions):\n",
    "    predicted_var_name = decode_predicted_tokens(prediction, batch, data_manager)\n",
    "    print(f\"  ({i + 1}) \", ' '.join(predicted_var_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "499cf9c699a428d4970219781bfcc814be552e7fef77f1ad041eb210673d5296"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
